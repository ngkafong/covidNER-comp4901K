{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-for-tf2 in /opt/conda/lib/python3.7/site-packages (0.14.7)\n",
      "Requirement already satisfied: params-flow>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from bert-for-tf2) (0.8.2)\n",
      "Requirement already satisfied: py-params>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from bert-for-tf2) (0.9.7)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from params-flow>=0.8.0->bert-for-tf2) (4.45.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.5)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-for-tf2\n",
    "#!pip install jax jaxlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys in train_dict: dict_keys(['id', 'word_seq', 'tag_seq'])\n",
      "keys in val_dict: dict_keys(['id', 'word_seq', 'tag_seq'])\n",
      "keys in test_dict: dict_keys(['id', 'word_seq'])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "#import numpy as np\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle as pkl\n",
    "\n",
    "train_dict = pkl.load(open(\"../input/comp4901k/train.pkl\", \"rb\"))\n",
    "val_dict = pkl.load(open(\"../input/comp4901k/val.pkl\", \"rb\"))\n",
    "test_dict = pkl.load(open(\"../input/comp4901k/test.pkl\", \"rb\"))\n",
    "\n",
    "print(\"keys in train_dict:\", train_dict.keys())\n",
    "print(\"keys in val_dict:\", val_dict.keys())\n",
    "print(\"keys in test_dict:\", test_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up BERT directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert\n",
    "\n",
    "bert_dir = \"../input/biobert-large/biobert_large\"\n",
    "bert_ckpt = os.path.join(bert_dir, \"bio_bert_large_1000k.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_params = bert.params_from_pretrained_ckpt(bert_dir)\n",
    "l_bert = bert.BertModelLayer.from_params(bert_params, name=\"bert\")\n",
    "l_bert.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert.tokenization.bert_tokenization import FullTokenizer\n",
    "tokenizer = FullTokenizer(vocab_file=os.path.join(bert_dir, \"vocab_cased_pubmed_pmc_30k.txt\"))\n",
    "\n",
    "def word2idx(word):\n",
    "    if word == '_w_pad_':\n",
    "        return 0\n",
    "    if word in tokenizer.vocab:\n",
    "        return tokenizer.vocab[word]\n",
    "    elif word.lower() in tokenizer.vocab:\n",
    "        return tokenizer.vocab[word.lower()]\n",
    "    else:\n",
    "        try:\n",
    "            return tokenizer.vocab[tokenizer.tokenize(word)[-1]]\n",
    "        except:\n",
    "            return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = np.vectorize(word2idx)(np.array(train_dict['word_seq']))\n",
    "val_tokens = np.vectorize(word2idx)(np.array(val_dict['word_seq']))\n",
    "test_tokens = np.vectorize(word2idx)(np.array(test_dict['word_seq']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_dict = {'_t_pad_': 0} # add a padding token\n",
    "\n",
    "for tag_seq in train_dict['tag_seq']:\n",
    "    for tag in tag_seq:\n",
    "        if(tag not in tag_dict):\n",
    "            tag_dict[tag] = len(tag_dict)\n",
    "\n",
    "tag2idx = tag_dict\n",
    "idx2tag = {v:k for k,v in tag2idx.items()} \n",
    "\n",
    "tag_dict_size = len(tag_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tags = [[tag2idx[t] for t in t_seq] for t_seq in train_dict['tag_seq']]\n",
    "train_tags = np.array([to_categorical(t_seq, num_classes=len(tag_dict)) for t_seq in train_tags])\n",
    "\n",
    "val_tags = [[tag2idx[t] for t in t_seq] for t_seq in val_dict['tag_seq']]\n",
    "val_tags = np.array([to_categorical(t_seq, num_classes=len(tag_dict)) for t_seq in val_tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided function to test accuracy\n",
    "# You could check the validation accuracy to select the best of your models\n",
    "def calc_accuracy(preds, tags, padding_id=\"_t_pad_\"):\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            preds (np.narray): (num_data, length_sentence)\n",
    "            tags  (np.narray): (num_data, length_sentence)\n",
    "        Output:\n",
    "            Proportion of correct prediction. The padding tokens are filtered out.\n",
    "    \"\"\"\n",
    "    preds_flatten = preds.flatten()\n",
    "    tags_flatten = tags.flatten()\n",
    "    non_padding_idx = np.where(tags_flatten!=padding_id)[0]\n",
    "    \n",
    "    return sum(preds_flatten[non_padding_idx]==tags_flatten[non_padding_idx])/len(non_padding_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_params = bert.params_from_pretrained_ckpt(bert_dir)\n",
    "l_bert = bert.BertModelLayer.from_params(bert_params, name=\"bert\")\n",
    "l_bert.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading 388 BERT weights from: ../input/biobert-large/biobert_large/bio_bert_large_1000k.ckpt into <bert.model.BertModelLayer object at 0x7f3c0ec64990> (prefix:bert_1). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tbert/embeddings/token_type_embeddings\n",
      "\tbert/pooler/dense/bias\n",
      "\tbert/pooler/dense/kernel\n",
      "\tglobal_step\n"
     ]
    }
   ],
   "source": [
    "with tpu_strategy.scope():\n",
    "    model = keras.models.Sequential([\n",
    "      keras.layers.InputLayer(input_shape=(128,)),\n",
    "      l_bert,\n",
    "      keras.layers.Dense(128, activation='relu'),\n",
    "      keras.layers.Dense(tag_dict_size, activation='softmax')\n",
    "    ])\n",
    "    model.build(input_shape=(None, 256))\n",
    "    bert.load_bert_weights(l_bert, bert_ckpt)\n",
    "    l_bert.apply_adapter_freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (BertModelLayer)        (None, 128, 1024)         363247616 \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128, 128)          131200    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128, 65)           8385      \n",
      "=================================================================\n",
      "Total params: 363,387,201\n",
      "Trainable params: 139,585\n",
      "Non-trainable params: 363,247,616\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tpu_strategy.scope():\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.02), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "24/24 [==============================] - 104s 4s/step - accuracy: 0.6916 - loss: 3.0664 - val_accuracy: 0.7630 - val_loss: 2.6054\n",
      "Epoch 2/60\n",
      "24/24 [==============================] - 12s 493ms/step - accuracy: 0.7756 - loss: 2.5614 - val_accuracy: 0.7835 - val_loss: 2.5000\n",
      "Epoch 3/60\n",
      "24/24 [==============================] - 12s 487ms/step - accuracy: 0.7828 - loss: 2.5153 - val_accuracy: 0.7868 - val_loss: 2.4800\n",
      "Epoch 4/60\n",
      "24/24 [==============================] - 12s 491ms/step - accuracy: 0.7844 - loss: 2.5064 - val_accuracy: 0.7893 - val_loss: 2.4717\n",
      "Epoch 5/60\n",
      "24/24 [==============================] - 12s 488ms/step - accuracy: 0.7856 - loss: 2.4761 - val_accuracy: 0.7906 - val_loss: 2.3642\n",
      "Epoch 6/60\n",
      "24/24 [==============================] - 12s 486ms/step - accuracy: 0.7874 - loss: 2.3948 - val_accuracy: 0.7905 - val_loss: 2.3590\n",
      "Epoch 7/60\n",
      "24/24 [==============================] - 12s 488ms/step - accuracy: 0.7886 - loss: 2.3878 - val_accuracy: 0.7725 - val_loss: 2.3970\n",
      "Epoch 8/60\n",
      "24/24 [==============================] - 12s 489ms/step - accuracy: 0.7739 - loss: 2.4632 - val_accuracy: 0.7804 - val_loss: 2.3851\n",
      "Epoch 9/60\n",
      "24/24 [==============================] - 12s 489ms/step - accuracy: 0.7859 - loss: 2.3941 - val_accuracy: 0.7943 - val_loss: 2.3468\n",
      "Epoch 10/60\n",
      "24/24 [==============================] - 12s 486ms/step - accuracy: 0.7739 - loss: 2.4147 - val_accuracy: 0.6612 - val_loss: 2.5646\n",
      "Epoch 11/60\n",
      "24/24 [==============================] - 12s 492ms/step - accuracy: 0.7696 - loss: 2.4615 - val_accuracy: 0.7853 - val_loss: 2.2163\n",
      "Epoch 12/60\n",
      "24/24 [==============================] - 12s 486ms/step - accuracy: 0.7765 - loss: 1.4304 - val_accuracy: 0.7844 - val_loss: 0.9775\n",
      "Epoch 13/60\n",
      "24/24 [==============================] - 12s 488ms/step - accuracy: 0.7894 - loss: 0.8788 - val_accuracy: 0.8009 - val_loss: 0.7909\n",
      "Epoch 14/60\n",
      "24/24 [==============================] - 12s 487ms/step - accuracy: 0.7987 - loss: 0.7797 - val_accuracy: 0.8081 - val_loss: 0.7345\n",
      "Epoch 15/60\n",
      "24/24 [==============================] - 12s 489ms/step - accuracy: 0.8054 - loss: 0.7395 - val_accuracy: 0.8140 - val_loss: 0.6986\n",
      "Epoch 16/60\n",
      "24/24 [==============================] - 12s 489ms/step - accuracy: 0.8091 - loss: 0.7179 - val_accuracy: 0.8166 - val_loss: 0.6847\n",
      "Epoch 17/60\n",
      "24/24 [==============================] - 12s 485ms/step - accuracy: 0.8109 - loss: 0.7066 - val_accuracy: 0.8191 - val_loss: 0.6693\n",
      "Epoch 18/60\n",
      "24/24 [==============================] - 12s 489ms/step - accuracy: 0.8123 - loss: 0.6976 - val_accuracy: 0.8199 - val_loss: 0.6638\n",
      "Epoch 19/60\n",
      "24/24 [==============================] - 12s 487ms/step - accuracy: 0.8134 - loss: 0.6921 - val_accuracy: 0.8202 - val_loss: 0.6638\n",
      "Epoch 20/60\n",
      "24/24 [==============================] - 12s 488ms/step - accuracy: 0.8141 - loss: 0.6882 - val_accuracy: 0.8224 - val_loss: 0.6517\n",
      "Epoch 21/60\n",
      "24/24 [==============================] - 12s 490ms/step - accuracy: 0.8150 - loss: 0.6837 - val_accuracy: 0.8231 - val_loss: 0.6487\n",
      "Epoch 22/60\n",
      "24/24 [==============================] - 12s 485ms/step - accuracy: 0.8156 - loss: 0.6807 - val_accuracy: 0.8238 - val_loss: 0.6442\n",
      "Epoch 23/60\n",
      "24/24 [==============================] - 12s 487ms/step - accuracy: 0.8162 - loss: 0.6768 - val_accuracy: 0.8244 - val_loss: 0.6425\n",
      "Epoch 24/60\n",
      "24/24 [==============================] - 12s 489ms/step - accuracy: 0.8163 - loss: 0.6763 - val_accuracy: 0.8246 - val_loss: 0.6423\n",
      "Epoch 25/60\n",
      "24/24 [==============================] - 12s 484ms/step - accuracy: 0.8164 - loss: 0.6747 - val_accuracy: 0.8252 - val_loss: 0.6385\n",
      "Epoch 26/60\n",
      "24/24 [==============================] - 12s 491ms/step - accuracy: 0.8171 - loss: 0.6722 - val_accuracy: 0.8256 - val_loss: 0.6383\n",
      "Epoch 27/60\n",
      "24/24 [==============================] - 12s 494ms/step - accuracy: 0.8172 - loss: 0.6709 - val_accuracy: 0.8255 - val_loss: 0.6359\n",
      "Epoch 28/60\n",
      "24/24 [==============================] - 12s 490ms/step - accuracy: 0.8175 - loss: 0.6701 - val_accuracy: 0.8263 - val_loss: 0.6355\n",
      "Epoch 29/60\n",
      "24/24 [==============================] - 12s 486ms/step - accuracy: 0.8177 - loss: 0.6697 - val_accuracy: 0.8258 - val_loss: 0.6385\n",
      "Epoch 30/60\n",
      "24/24 [==============================] - 12s 486ms/step - accuracy: 0.8174 - loss: 0.6710 - val_accuracy: 0.8255 - val_loss: 0.6363\n",
      "Epoch 31/60\n",
      "24/24 [==============================] - 12s 488ms/step - accuracy: 0.8181 - loss: 0.6689 - val_accuracy: 0.8264 - val_loss: 0.6328\n",
      "Epoch 32/60\n",
      "24/24 [==============================] - 12s 487ms/step - accuracy: 0.8181 - loss: 0.6669 - val_accuracy: 0.8267 - val_loss: 0.6322\n",
      "Epoch 33/60\n",
      "24/24 [==============================] - 12s 488ms/step - accuracy: 0.8184 - loss: 0.6661 - val_accuracy: 0.8263 - val_loss: 0.6347\n",
      "Epoch 34/60\n",
      "24/24 [==============================] - 12s 486ms/step - accuracy: 0.8183 - loss: 0.6660 - val_accuracy: 0.8267 - val_loss: 0.6333\n",
      "Epoch 35/60\n",
      "24/24 [==============================] - 12s 486ms/step - accuracy: 0.8188 - loss: 0.6651 - val_accuracy: 0.8264 - val_loss: 0.6316\n",
      "Epoch 36/60\n",
      "24/24 [==============================] - 12s 491ms/step - accuracy: 0.8188 - loss: 0.6640 - val_accuracy: 0.8267 - val_loss: 0.6305\n",
      "Epoch 37/60\n",
      "24/24 [==============================] - 12s 490ms/step - accuracy: 0.8193 - loss: 0.6626 - val_accuracy: 0.8270 - val_loss: 0.6331\n",
      "Epoch 38/60\n",
      "24/24 [==============================] - 12s 488ms/step - accuracy: 0.8188 - loss: 0.6648 - val_accuracy: 0.8278 - val_loss: 0.6268\n",
      "Epoch 39/60\n",
      "24/24 [==============================] - 12s 486ms/step - accuracy: 0.8195 - loss: 0.6609 - val_accuracy: 0.8281 - val_loss: 0.6266\n",
      "Epoch 40/60\n",
      "24/24 [==============================] - 12s 487ms/step - accuracy: 0.8200 - loss: 0.6596 - val_accuracy: 0.8282 - val_loss: 0.6264\n",
      "Epoch 41/60\n",
      "24/24 [==============================] - 12s 486ms/step - accuracy: 0.8200 - loss: 0.6594 - val_accuracy: 0.8284 - val_loss: 0.6264\n",
      "Epoch 42/60\n",
      "24/24 [==============================] - 12s 486ms/step - accuracy: 0.8204 - loss: 0.6590 - val_accuracy: 0.8288 - val_loss: 0.6249\n",
      "Epoch 43/60\n",
      "24/24 [==============================] - 12s 487ms/step - accuracy: 0.8204 - loss: 0.6584 - val_accuracy: 0.8289 - val_loss: 0.6239\n",
      "Epoch 44/60\n",
      "24/24 [==============================] - 12s 493ms/step - accuracy: 0.8206 - loss: 0.6572 - val_accuracy: 0.8290 - val_loss: 0.6231\n",
      "Epoch 45/60\n",
      "24/24 [==============================] - 12s 487ms/step - accuracy: 0.8206 - loss: 0.6583 - val_accuracy: 0.8295 - val_loss: 0.6237\n",
      "Epoch 46/60\n",
      "24/24 [==============================] - 12s 490ms/step - accuracy: 0.8210 - loss: 0.6556 - val_accuracy: 0.8297 - val_loss: 0.6197\n",
      "Epoch 47/60\n",
      "24/24 [==============================] - 12s 491ms/step - accuracy: 0.8214 - loss: 0.6539 - val_accuracy: 0.8294 - val_loss: 0.6248\n",
      "Epoch 48/60\n",
      "24/24 [==============================] - 12s 488ms/step - accuracy: 0.8209 - loss: 0.6560 - val_accuracy: 0.8277 - val_loss: 0.6276\n",
      "Epoch 49/60\n",
      "24/24 [==============================] - 12s 485ms/step - accuracy: 0.8213 - loss: 0.6549 - val_accuracy: 0.8303 - val_loss: 0.6162\n",
      "Epoch 50/60\n",
      "24/24 [==============================] - 12s 488ms/step - accuracy: 0.8218 - loss: 0.6528 - val_accuracy: 0.8307 - val_loss: 0.6171\n",
      "Epoch 51/60\n",
      "24/24 [==============================] - 12s 487ms/step - accuracy: 0.8220 - loss: 0.6514 - val_accuracy: 0.8308 - val_loss: 0.6152\n",
      "Epoch 52/60\n",
      "24/24 [==============================] - 12s 487ms/step - accuracy: 0.8221 - loss: 0.6507 - val_accuracy: 0.8303 - val_loss: 0.6166\n",
      "Epoch 53/60\n",
      "24/24 [==============================] - 12s 493ms/step - accuracy: 0.8222 - loss: 0.6507 - val_accuracy: 0.8305 - val_loss: 0.6158\n",
      "Epoch 54/60\n",
      "24/24 [==============================] - 12s 494ms/step - accuracy: 0.8222 - loss: 0.6501 - val_accuracy: 0.8307 - val_loss: 0.6144\n",
      "Epoch 55/60\n",
      "24/24 [==============================] - 13s 529ms/step - accuracy: 0.8225 - loss: 0.6492 - val_accuracy: 0.8312 - val_loss: 0.6146\n",
      "Epoch 56/60\n",
      "24/24 [==============================] - 12s 493ms/step - accuracy: 0.8224 - loss: 0.6492 - val_accuracy: 0.8311 - val_loss: 0.6145\n",
      "Epoch 57/60\n",
      "24/24 [==============================] - 12s 490ms/step - accuracy: 0.8229 - loss: 0.6478 - val_accuracy: 0.8299 - val_loss: 0.6191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/60\n",
      "24/24 [==============================] - 12s 485ms/step - accuracy: 0.8224 - loss: 0.6507 - val_accuracy: 0.8317 - val_loss: 0.6140\n",
      "Epoch 59/60\n",
      "24/24 [==============================] - 12s 487ms/step - accuracy: 0.8230 - loss: 0.6481 - val_accuracy: 0.8291 - val_loss: 0.6211\n",
      "Epoch 60/60\n",
      "24/24 [==============================] - 12s 486ms/step - accuracy: 0.8227 - loss: 0.6487 - val_accuracy: 0.8322 - val_loss: 0.6114\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 60\n",
    "# with tpu_strategy.scope():\n",
    "history = model.fit(train_tokens, \n",
    "                train_tags, \n",
    "                epochs=num_epochs, \n",
    "                batch_size=1024,\n",
    "                validation_data=(val_tokens, val_tags), \n",
    "               # callbacks=[EarlyStopping(monitor='val_accuracy', patience=6, min_delta=0.0001, restore_best_weights=True)]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_t_pad_': 0,\n",
       " 'O': 1,\n",
       " 'LIVESTOCK': 2,\n",
       " 'DISEASE_OR_SYNDROME': 3,\n",
       " 'GENE_OR_GENOME': 4,\n",
       " 'CARDINAL': 5,\n",
       " 'CHEMICAL': 6,\n",
       " 'PRODUCT': 7,\n",
       " 'QUANTITY': 8,\n",
       " 'NORP': 9,\n",
       " 'THERAPEUTIC_OR_PREVENTIVE_PROCEDURE': 10,\n",
       " 'CELL': 11,\n",
       " 'ORGANISM': 12,\n",
       " 'GROUP': 13,\n",
       " 'ORDINAL': 14,\n",
       " 'GPE': 15,\n",
       " 'ORG': 16,\n",
       " 'LABORATORY_PROCEDURE': 17,\n",
       " 'DATE': 18,\n",
       " 'CORONAVIRUS': 19,\n",
       " 'EUKARYOTE': 20,\n",
       " 'SIGN_OR_SYMPTOM': 21,\n",
       " 'VIRUS': 22,\n",
       " 'CELL_COMPONENT': 23,\n",
       " 'MOLECULAR_FUNCTION': 24,\n",
       " 'CELL_OR_MOLECULAR_DYSFUNCTION': 25,\n",
       " 'VIRAL_PROTEIN': 26,\n",
       " 'HUMAN-CAUSED_PHENOMENON_OR_PROCESS': 27,\n",
       " 'BODY_PART_ORGAN_OR_ORGAN_COMPONENT': 28,\n",
       " 'PERSON': 29,\n",
       " 'TISSUE': 30,\n",
       " 'RESEARCH_ACTIVITY': 31,\n",
       " 'EVENT': 32,\n",
       " 'IMMUNE_RESPONSE': 33,\n",
       " 'ORGAN_OR_TISSUE_FUNCTION': 34,\n",
       " 'MATERIAL': 35,\n",
       " 'EVOLUTION': 36,\n",
       " 'LABORATORY_OR_TEST_RESULT': 37,\n",
       " 'BACTERIUM': 38,\n",
       " 'MONEY': 39,\n",
       " 'FAC': 40,\n",
       " 'DAILY_OR_RECREATIONAL_ACTIVITY': 41,\n",
       " 'ANATOMICAL_STRUCTURE': 42,\n",
       " 'CELL_FUNCTION': 43,\n",
       " 'SUBSTRATE': 44,\n",
       " 'INDIVIDUAL_BEHAVIOR': 45,\n",
       " 'BODY_SUBSTANCE': 46,\n",
       " 'SOCIAL_BEHAVIOR': 47,\n",
       " 'WILDLIFE': 48,\n",
       " 'LOC': 49,\n",
       " 'LAW': 50,\n",
       " 'INJURY_OR_POISONING': 51,\n",
       " 'DIAGNOSTIC_PROCEDURE': 52,\n",
       " 'TIME': 53,\n",
       " 'EXPERIMENTAL_MODEL_OF_DISEASE': 54,\n",
       " 'GOVERNMENTAL_OR_REGULATORY_ACTIVITY': 55,\n",
       " 'PERCENT': 56,\n",
       " 'FOOD': 57,\n",
       " 'WORK_OF_ART': 58,\n",
       " 'MACHINE_ACTIVITY': 59,\n",
       " 'LANGUAGE': 60,\n",
       " 'EDUCATIONAL_ACTIVITY': 61,\n",
       " 'GROUP_ATTRIBUTE': 62,\n",
       " 'PHYSICAL_SCIENCE': 63,\n",
       " 'ARCHAEON': 64}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
